{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Financial News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the following, we will explore data modeling to predict numbers of positive, neutral and negative sentiments of the reviews.\n",
    "- For each review, which forms the independent variable, one of the three sentiments was assigned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop at 0x1cf3ac6ccc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('all-data.csv', names=['sentiment','text'], encoding='latin-1',header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>positive</td>\n",
       "      <td>TeliaSonera TLSN said the offer is in line wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>positive</td>\n",
       "      <td>STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>positive</td>\n",
       "      <td>A purchase agreement for 7,200 tons of gasolin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive</td>\n",
       "      <td>Finnish Talentum reports its operating profit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>positive</td>\n",
       "      <td>Clothing retail chain Sepp+Æl+Æ 's sales incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>positive</td>\n",
       "      <td>Consolidated net sales increased 16 % to reach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>positive</td>\n",
       "      <td>Foundries division reports its sales increased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>positive</td>\n",
       "      <td>HELSINKI ( AFX ) - Shares closed higher , led ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>positive</td>\n",
       "      <td>Incap Contract Manufacturing Services Pvt Ltd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>positive</td>\n",
       "      <td>Its board of directors will propose a dividend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0    neutral  According to Gran , the company has no plans t...\n",
       "1    neutral  Technopolis plans to develop in stages an area...\n",
       "2   negative  The international electronic industry company ...\n",
       "3   positive  With the new production plant the company woul...\n",
       "4   positive  According to the company 's updated strategy f...\n",
       "5   positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n",
       "6   positive  For the last quarter of 2010 , Componenta 's n...\n",
       "7   positive  In the third quarter of 2010 , net sales incre...\n",
       "8   positive  Operating profit rose to EUR 13.1 mn from EUR ...\n",
       "9   positive  Operating profit totalled EUR 21.1 mn , up fro...\n",
       "10  positive  TeliaSonera TLSN said the offer is in line wit...\n",
       "11  positive  STORA ENSO , NORSKE SKOG , M-REAL , UPM-KYMMEN...\n",
       "12  positive  A purchase agreement for 7,200 tons of gasolin...\n",
       "13  positive  Finnish Talentum reports its operating profit ...\n",
       "14  positive  Clothing retail chain Sepp+Æl+Æ 's sales incre...\n",
       "15  positive  Consolidated net sales increased 16 % to reach...\n",
       "16  positive  Foundries division reports its sales increased...\n",
       "17  positive  HELSINKI ( AFX ) - Shares closed higher , led ...\n",
       "18  positive  Incap Contract Manufacturing Services Pvt Ltd ...\n",
       "19  positive  Its board of directors will propose a dividend..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4846</td>\n",
       "      <td>4846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>4838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Ahlstrom 's share is quoted on the NASDAQ OMX ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2879</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                               text\n",
       "count       4846                                               4846\n",
       "unique         3                                               4838\n",
       "top      neutral  Ahlstrom 's share is quoted on the NASDAQ OMX ...\n",
       "freq        2879                                                  2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are total 4846 rows \n",
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4846 entries, 0 to 4845\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  4846 non-null   object\n",
      " 1   text       4846 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 75.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     2879\n",
       "positive    1363\n",
       "negative     604\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of values in the target variable.\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We notice that the classification is not balanced \n",
    "CountStatus = pd.value_counts(data['sentiment'].values, sort=True)\n",
    "CountStatus.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the duplicated rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the duplicated rows and eliminating them \n",
    "len(data[data.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4840, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the reqruied NLTK toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raehy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raehy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanining the texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the square brakets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "  pattern=r'[^a-zA-z0-9\\s]'\n",
    "  text=re.sub(pattern,'',text) \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substituting multiple spaces with single space \n",
    "def sub_multi_space (text):\n",
    "    text= re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_special_characters(text, remove_digits=True)\n",
    "    text = sub_multi_space (text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function on review column\n",
    "data['text']=data['text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the target variable classes in numeric type\n",
    "label_mapping = {\n",
    "        'negative': 0,\n",
    "        'neutral': 1,\n",
    "        'positive': 2\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment']=data['sentiment'].replace(label_mapping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran the company has no plans to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>According to the company s updated strategy fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  According to Gran the company has no plans to ...\n",
       "1          1  Technopolis plans to develop in stages an area...\n",
       "2          0  The international electronic industry company ...\n",
       "3          2  With the new production plant the company woul...\n",
       "4          2  According to the company s updated strategy fo..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\raehy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'which', 'that', \"shan't\", \"weren't\", 'if', 'just', 'should', \"wasn't\", 'can', 'i', 'what', 'above', 'mightn', 'so', 'mustn', \"shouldn't\", \"hasn't\", 'weren', 'will', 'isn', 'up', 'wouldn', \"needn't\", 'because', \"aren't\", 'to', 'have', 'yourselves', 'hers', 'own', 'were', \"couldn't\", 'myself', 'but', 'her', 'his', \"she's\", 'only', 'ourselves', 'ain', 'and', 'against', 'wasn', 'are', 'through', 'has', 'be', 'themselves', 'itself', 'where', 'he', \"didn't\", \"wouldn't\", 'below', 'these', 'too', 'between', \"won't\", 'during', 'shan', 'once', 'then', 'very', \"should've\", 'yours', 'o', 'of', 'not', 'into', 'shouldn', 'don', 'himself', 'when', 'him', 'after', 'your', 'them', 've', \"it's\", 'each', \"you've\", 'over', 'how', 'than', 'whom', 'do', 'herself', 'won', \"mustn't\", 'while', \"doesn't\", 'did', 'my', 'd', 'those', 'a', 'in', 'with', \"that'll\", 'ma', 'having', 'had', 'off', 'its', 'aren', 'why', 'she', 'about', \"you're\", 'didn', 'this', 'll', 'doesn', 'hadn', 'an', 'more', 'ours', 'their', 't', \"mightn't\", 'again', 'our', 'here', \"you'd\", 'such', 'being', 'or', 'was', 'they', \"don't\", 'the', 'some', 'does', 'doing', 'theirs', \"haven't\", 'until', 'am', 'under', 'yourself', 'few', 'been', 'any', 'couldn', 'is', 'for', 'nor', 'from', 'down', 'on', 'me', 'who', 'y', 'by', 'we', 'at', 'm', 'now', 'no', 'there', 'before', 're', 'most', 'haven', \"isn't\", \"you'll\", 'same', 'you', 'further', 's', 'out', 'other', 'it', 'needn', 'hasn', 'both', 'all', 'as', \"hadn't\"}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "data['text']=data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization is a linguistic term that means grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. The aim is to take away inflectional suffixes and prefixes to bring out the word’s dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_text(text):\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "data['text']=data['text'].apply(lambda x: lemma_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    According Gran company plan move production Ru...\n",
       "1    Technopolis plan develop stage area le 100000 ...\n",
       "2    international electronic industry company Elco...\n",
       "3    new production plant company would increase ca...\n",
       "4    According company update strategy year 2009201...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(data['text'],\n",
    "                     min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_weights = w2v_model.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 65 - Embedding Dim: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raehy\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1099",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1099",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-67ae8eff7302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwordvec_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mwordvec_arrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwordvec_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordvec_arrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwordvec_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1099"
     ]
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(data['text']), 300)) \n",
    "for i in range(len(data['text'])):\n",
    "    wordvec_arrays[i,:] = word_vector(data['text'][i], 300)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized train ['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Technopolis plan develop stage area le 100000 square meter order host company work computer technology telecommunication statement say'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train text\n",
    "norm_train_text=data.text[:4845]\n",
    "norm_train_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bags of Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text data must be converted to a real-valued numeric vector. The two common approaches are Count Occurence and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer()\n",
    "#transformed train reviews\n",
    "X_train_counts = cv.fit_transform(norm_train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (4840, 9698)\n"
     ]
    }
   ],
   "source": [
    "print('BOW_cv_train:', X_train_counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tfidf Transformer\n",
    "tf=TfidfTransformer()\n",
    "#transformed train reviews\n",
    "X_train_tfidf =tf.fit_transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (4840, 9698)\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf_train:',X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modelling \n",
    "\n",
    "#### Logistics Regression \n",
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X_train_tfidf,data['sentiment'].values, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline ([('vect', tf),('chi', SelectKBest(chi2, k=1200)), ('clf',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting our model and save it in a pickle for later use\n",
    "model = pipeline.fit(X_train_tfidf, data['sentiment'].values)\n",
    "with open('LogisticRegression.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "ytest = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.41      0.57       166\n",
      "           1       0.74      0.97      0.84       872\n",
      "           2       0.85      0.49      0.62       414\n",
      "\n",
      "    accuracy                           0.77      1452\n",
      "   macro avg       0.84      0.62      0.68      1452\n",
      "weighted avg       0.79      0.77      0.75      1452\n",
      "\n",
      "[[ 68  85  13]\n",
      " [  2 846  24]\n",
      " [  2 210 202]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, model.predict(X_test)))\n",
    "print(confusion_matrix(ytest, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 78% of accuracy is not considered highly effective. Therefore, I will try with Random Forest Classifier Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline ([('vect', tf),('chi', SelectKBest(chi2, k=1200)), ('clf',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting our model and save it in a pickle for later use\n",
    "model = pipeline.fit(X_train_tfidf, data['sentiment'].values)\n",
    "with open('RandomForest.pickle', 'wb') as g:\n",
    "    pickle.dump(model, g)\n",
    "ytest = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       166\n",
      "           1       0.99      1.00      0.99       872\n",
      "           2       1.00      0.97      0.98       414\n",
      "\n",
      "    accuracy                           0.99      1452\n",
      "   macro avg       0.99      0.99      0.99      1452\n",
      "weighted avg       0.99      0.99      0.99      1452\n",
      "\n",
      "[[165   1   0]\n",
      " [  0 871   1]\n",
      " [  0  12 402]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, model.predict(X_test)))\n",
    "print(confusion_matrix(ytest, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The acccuracy rate of 99% implies that the model may overfit the training dataset. In hope of closing the gap between under nad overfitting, let's give a shot to Artificial Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data to train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_tfidf.toarray()\n",
    "y = (np.array(data['sentiment'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure the dataset sizes of train and test are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (3872, 9698), \n",
      "Test dataset shape: (968, 9698)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (3872,), \n",
      "Test dataset shape: (968,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(Y_train.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a function that returns the appropriate number of units and the activation for the last layer.\n",
    "\n",
    "def get_last_layer_units_and_activation(num_classes):\n",
    "    if num_classes == 3:\n",
    "        activation = 'softmax'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'relu'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 9698)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4965888   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 5,229,057\n",
      "Trainable params: 5,229,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT_RATE = 0.2\n",
    "UNITS = 512\n",
    "NUM_CLASSES = 3\n",
    "LAYERS = 3\n",
    "input_shape = X_train_tfidf.shape[1:]\n",
    "\n",
    "\n",
    "op_units, op_activation = get_last_layer_units_and_activation(NUM_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Applies Dropout to the input\n",
    "model.add(Dropout(rate=DROPOUT_RATE, input_shape=input_shape))\n",
    "for _ in range(LAYERS-1):\n",
    "    model.add(Dense(units=UNITS, activation='relu'))\n",
    "    model.add(Dropout(rate=DROPOUT_RATE))\n",
    "    \n",
    "model.add(Dense(units=op_units, activation=op_activation))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfTransformer' object has no attribute 'optimizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-007f4f81e4bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfTransformer' object has no attribute 'optimizers'"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Compile model with parameters\n",
    "if NUM_CLASSES == 3:\n",
    "    loss = 'categorical_crossentropy'\n",
    "else:\n",
    "    loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = tf.optimizers.Adam(Lr=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=EPOCHS, validation_data=(X_test, Y_test), verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaludate the Model + Plotting Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs=range(len(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(epochs,acc,label='Training_acc', color='blue')\n",
    "plt.plot(epochs,val_acc,label='Validation_acc',color='red')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss\n",
    "plt.plot(epochs,loss,label='Training_loss', color='blue')\n",
    "plt.plot(epochs,val_loss,label='Validation_loss',color='red')\n",
    "plt.legend()\n",
    "plt.title('Training and Validaiton loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
